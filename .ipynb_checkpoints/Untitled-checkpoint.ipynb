{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script name: /home/arac/miniconda3/envs/gluon/lib/python3.6/site-packages/ipykernel_launcher.py\n",
      "total chars: 71\n",
      "Sample doc['\"the premise is amazing and the some of the acting, notably sally kellerman and anthony rapp, is charming...', 'but this film is near unwatchable.', 'the music sounds as if it comes from some sort of the royalty free online site and the lyrics as if they were written with a rhyming dictionary open on the lap.', 'most of the singing is off-key.', 'i think they may have filmed with the singing accapella and put in the music under it...', 'the dialogue is really stupid and trite.', 'the movie works best when it is actually talking about the real estate but unfortunately it strays to often into stupid farcical sub-plots.', 'i found myself checking my watch after ther first twenty minutes and after 40 wondering \\'when is it ever going to end.\\'\"']\n",
      "Sample X:[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 26  3 51 29 43  0 30 23 43 68  1 43 68  0  3  1 43 15 14 51 68  1 17\n",
      " 23 59 17 55 43 19 68 17 50 15 30 14 43 51 43 30 23 17 39 43 68  0 23 23 17\n",
      " 14 39 43  0 14  0 39 43 15  0 30 23 43  9 17 43 32 51 43 32 59 17 14 15 29\n",
      " 43  0 30 23 43 55 68 51 43  0 23 17 32 43  0 68 17 29 68  1 43  0  0 14  9\n",
      " 43 15 23 29 51 15  1 14 43  0 30 23 43  9  1 43 23 14  1 32 43  0 50  1 32\n",
      " 43 50  1 14  9 43 32  0 50  1 59 43 23 17 43  9 17 43 32 51 43 32 55 68 66\n",
      "  1 32 43 59 17 32 66 50 43  0 30 23]\n",
      "y:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling1D, Conv1D, GlobalMaxPool1D\n",
    "from keras.layers import LSTM, Lambda, Bidirectional, concatenate, BatchNormalization\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import keras.callbacks\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "def binarize(x, sz=71):\n",
    "    return tf.to_float(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1))\n",
    "\n",
    "\n",
    "def binarize_outshape(in_shape):\n",
    "    return in_shape[0], in_shape[1], 71\n",
    "\n",
    "\n",
    "def striphtml(s):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', s)\n",
    "\n",
    "\n",
    "def clean(s):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', s)\n",
    "\n",
    "\n",
    "total = len(sys.argv)\n",
    "cmdargs = str(sys.argv)\n",
    "\n",
    "print (\"Script name: %s\" % str(sys.argv[0]))\n",
    "checkpoint = None\n",
    "if len(sys.argv) == 2:\n",
    "    if os.path.exists(str(sys.argv[1])):\n",
    "        print (\"Checkpoint : %s\" % str(sys.argv[1]))\n",
    "        checkpoint = str(sys.argv[1])\n",
    "\n",
    "data = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "txt = ''\n",
    "docs = []\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "for cont, sentiment in zip(data.review, data.sentiment):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', clean(striphtml(cont)))\n",
    "    sentences = [sent.lower() for sent in sentences]\n",
    "    docs.append(sentences)\n",
    "    sentiments.append(sentiment)\n",
    "\n",
    "num_sent = []\n",
    "for doc in docs:\n",
    "    num_sent.append(len(doc))\n",
    "    for s in doc:\n",
    "        txt += s\n",
    "\n",
    "chars = set(txt)\n",
    "\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print('Sample doc{}'.format(docs[1200]))\n",
    "\n",
    "maxlen = 512\n",
    "max_sentences = 15\n",
    "\n",
    "X = np.ones((len(docs), max_sentences, maxlen), dtype=np.int64) * -1\n",
    "y = np.array(sentiments)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for j, sentence in enumerate(doc):\n",
    "        if j < max_sentences:\n",
    "            for t, char in enumerate(sentence[-maxlen:]):\n",
    "                X[i, j, (maxlen - 1 - t)] = char_indices[char]\n",
    "\n",
    "print('Sample X:{}'.format(X[1200, 2]))\n",
    "print('y:{}'.format(y[1200]))\n",
    "\n",
    "ids = np.arange(len(X))\n",
    "np.random.shuffle(ids)\n",
    "\n",
    "# shuffle\n",
    "X = X[ids]\n",
    "y = y[ids]\n",
    "\n",
    "X_train = X[:20000]\n",
    "X_test = X[22500:]\n",
    "\n",
    "y_train = y[:20000]\n",
    "y_test = y[22500:]\n",
    "\n",
    "\n",
    "def char_block(in_layer, nb_filter=(64, 100), filter_length=(3, 3), subsample=(2, 1), pool_length=(2, 2)):\n",
    "    block = in_layer\n",
    "    for i in range(len(nb_filter)):\n",
    "\n",
    "        block = Conv1D(filters=nb_filter[i],\n",
    "                       kernel_size=filter_length[i],\n",
    "                       padding='valid',\n",
    "                       activation='tanh',\n",
    "                       strides=subsample[i])(block)\n",
    "\n",
    "        # block = BatchNormalization()(block)\n",
    "        # block = Dropout(0.1)(block)\n",
    "        if pool_length[i]:\n",
    "            block = MaxPooling1D(pool_size=pool_length[i])(block)\n",
    "\n",
    "    # block = Lambda(max_1d, output_shape=(nb_filter[-1],))(block)\n",
    "    block = GlobalMaxPool1D()(block)\n",
    "    block = Dense(128, activation='relu')(block)\n",
    "    return block\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 512)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 512, 71)       0           input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)                (None, 508, 128)      45568       lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)                (None, 506, 192)      95616       lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)   (None, 254, 128)      0           conv1d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)   (None, 253, 192)      0           conv1d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)                (None, 250, 256)      164096      max_pooling1d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)                (None, 249, 320)      307520      max_pooling1d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)   (None, 125, 256)      0           conv1d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)   (None, 124, 320)      0           conv1d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalMa (None, 256)           0           max_pooling1d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalMa (None, 320)           0           max_pooling1d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 128)           32896       global_max_pooling1d_3[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 128)           41088       global_max_pooling1d_4[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 256)           0           dense_3[0][0]                    \n",
      "                                                                   dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 686,784\n",
      "Trainable params: 686,784\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_features = len(chars) + 1\n",
    "char_embedding = 40\n",
    "\n",
    "document = Input(shape=(max_sentences, maxlen), dtype='int64')\n",
    "in_sentence = Input(shape=(maxlen,), dtype='int64')\n",
    "\n",
    "embedded = Lambda(binarize, output_shape=binarize_outshape)(in_sentence)\n",
    "\n",
    "block2 = char_block(embedded, (128, 256), filter_length=(5, 5), subsample=(1, 1), pool_length=(2, 2))\n",
    "block3 = char_block(embedded, (192, 320), filter_length=(7, 5), subsample=(1, 1), pool_length=(2, 2))\n",
    "\n",
    "sent_encode = concatenate([block2, block3], axis=-1)\n",
    "# sent_encode = Dropout(0.2)(sent_encode)\n",
    "\n",
    "encoder = Model(inputs=in_sentence, outputs=sent_encode)\n",
    "encoder.summary()\n",
    "\n",
    "encoded = TimeDistributed(encoder)(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_h = 92\n",
    "\n",
    "lstm_layer = LSTM(lstm_h, return_sequences=True, dropout=0.1, recurrent_dropout=0.1, implementation=0)(encoded)\n",
    "lstm_layer2 = LSTM(lstm_h, return_sequences=False, dropout=0.1, recurrent_dropout=0.1, implementation=0)(lstm_layer)\n",
    "\n",
    "# output = Dropout(0.2)(bi_lstm)\n",
    "output = Dense(1, activation='sigmoid')(lstm_layer2)\n",
    "\n",
    "model = Model(outputs=output, inputs=document)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if checkpoint:\n",
    "    model.load_weights(checkpoint)\n",
    "\n",
    "file_name = os.path.basename(sys.argv[0]).split('.')[0]\n",
    "\n",
    "check_cb = keras.callbacks.ModelCheckpoint('checkpoints/' + file_name + '.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                           monitor='val_loss',\n",
    "                                           verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "optimizer = 'rmsprop'\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=10, epochs=30, shuffle=True, callbacks=[check_cb, earlystop_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling1D, Conv1D\n",
    "from keras.layers import LSTM, Lambda\n",
    "from keras.layers import TimeDistributed, Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import keras.callbacks\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "txt = ''\n",
    "docs = []\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "def binarize_outshape(in_shape):\n",
    "    return in_shape[0], in_shape[1], 71\n",
    "\n",
    "\n",
    "def striphtml(html):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', html)\n",
    "\n",
    "def clean(s):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', s)\n",
    "\n",
    "for cont, sentiment in zip(data.review, data.sentiment):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', clean(striphtml(cont)))\n",
    "    sentences = [sent.lower() for sent in sentences]\n",
    "    docs.append(sentences)\n",
    "    sentiments.append(sentiment)\n",
    "    \n",
    "for doc in docs:\n",
    "    for s in doc:\n",
    "        txt += s\n",
    "chars = set(txt)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "maxlen = 512\n",
    "max_sentences = 15\n",
    "\n",
    "X = np.ones((len(docs), max_sentences, maxlen), dtype=np.int64) * -1\n",
    "y = np.array(sentiments)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for j, sentence in enumerate(doc):\n",
    "        if j < max_sentences:\n",
    "            for t, char in enumerate(sentence[-maxlen:]):\n",
    "                X[i, j, (maxlen-1-t)] = char_indices[char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(x, sz=71):\n",
    "    return tf.to_float(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1))\n",
    "\n",
    "filter_length = [5, 3, 3]\n",
    "nb_filter = [196, 196, 256]\n",
    "pool_length = 2\n",
    "\n",
    "in_sentence = Input(shape=(maxlen,), dtype='int64')\n",
    "# binarize function creates a onehot encoding of each character index\n",
    "embedded = Lambda(binarize, output_shape=binarize_outshape)(in_sentence)\n",
    "\n",
    "for i in range(len(nb_filter)):\n",
    "    embedded = Conv1D(nb_filter=nb_filter[i],\n",
    "                            filter_length=filter_length[i],\n",
    "                            border_mode='valid',\n",
    "                            activation='relu',\n",
    "                            init='glorot_normal',\n",
    "                            subsample_length=1)(embedded)\n",
    "\n",
    "    embedded = Dropout(0.1)(embedded)\n",
    "    embedded = MaxPooling1D(pool_length=pool_length)(embedded)\n",
    "\n",
    "forward_sent = LSTM(128, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu')(embedded)\n",
    "backward_sent = LSTM(128, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu', go_backwards=True)(embedded)\n",
    "\n",
    "sent_encode = merge([forward_sent, backward_sent], mode='concat', concat_axis=-1)\n",
    "sent_encode = Dropout(0.3)(sent_encode)\n",
    "\n",
    "encoder = Model(input=in_sentence, output=sent_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_sent = LSTM(128, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu')(embedded)\n",
    "backward_sent = LSTM(128, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu', go_backwards=True)(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = Input(shape=(max_sentences, maxlen), dtype='int64')\n",
    "encoded = TimeDistributed(encoder)(sequence)\n",
    "forwards = LSTM(80, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu')(encoded)\n",
    "backwards = LSTM(80, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu', go_backwards=True)(encoded)\n",
    "\n",
    "merged = merge([forwards, backwards], mode='concat', concat_axis=-1)\n",
    "output = Dropout(0.3)(merged)\n",
    "output = Dense(128, activation='relu')(output)\n",
    "output = Dropout(0.3)(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(input=sequence, output=output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
