{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class CharNet(object):\n",
    "    \"\"\"docstring for CharNet.\"\"\"\n",
    "    def __init__(self, conv_layers,\n",
    "                        fc_layers,\n",
    "                        l0,\n",
    "                        alphabet_size,\n",
    "                        encoder,\n",
    "                        **args\n",
    "    ):\n",
    "        super(CharNet, self).__init__()\n",
    "        tf.set_random_seed(time.time())\n",
    "        self.l0 = l0\n",
    "        self.conv_layers = conv_layers\n",
    "        self.fc_layers = fc_layers\n",
    "        self.alphabet_size = alphabet_size\n",
    "\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        with tf.name_scope('Input'):\n",
    "            self.input_num = tf.placeholder(tf.float32, shape=[None, 6],\n",
    "                                           name='input_num')\n",
    "            self.input_x = tf.placeholder(tf.int64, shape=[None, self.l0],\n",
    "                                          name='input_x')\n",
    "            self.input_y = tf.placeholder(tf.float32, shape=[None, 1],\n",
    "                                          name='input_y')\n",
    "            self.dropout_keep_prob = tf.placeholder(tf.float32,\n",
    "                                                    name='dropout_keep_prob')\n",
    "\n",
    "        with tf.name_scope('Embedding'), tf.device('/cpu:0'):\n",
    "            x = tf.nn.embedding_lookup(encoder, self.input_x)\n",
    "            x = tf.expand_dims(x, -1)\n",
    "\n",
    "        # Configure conv layers\n",
    "        for i, layer_params in enumerate(conv_layers):\n",
    "            with tf.name_scope(\"Convolution\"):\n",
    "                filter_param = [\n",
    "                    layer_params[1],\n",
    "                    x.get_shape()[2].value, # l0\n",
    "                    x.get_shape()[3].value, # channels\n",
    "                    layer_params[0]\n",
    "                ]\n",
    "                W = tf.Variable(initializer(filter_param), dtype='float32', name='filter')\n",
    "\n",
    "                conv_layer = tf.nn.conv2d(x, W, [1, 1, 1, 1], 'VALID', name='conv')\n",
    "                conv_layer = tf.nn.relu(conv_layer, name='act_relu')\n",
    "\n",
    "            if not layer_params[-1] is None:\n",
    "                with tf.name_scope(\"MaxPooling\"):\n",
    "                    pool_layer = tf.nn.max_pool(conv_layer,\n",
    "                                            ksize=[1, layer_params[-1], 1, 1],\n",
    "                                            strides=[1, layer_params[-1], 1, 1],\n",
    "                                            padding='VALID')\n",
    "                    x = tf.transpose(pool_layer, [0, 1, 3, 2])\n",
    "            else:\n",
    "                x = tf.transpose(conv_layer, [0, 1, 3, 2])\n",
    "\n",
    "        # flatten conv output for fc\n",
    "        with tf.name_scope(\"Flatten\"):\n",
    "            x = tf.contrib.layers.flatten(x)\n",
    "\n",
    "        with tf.name_scope(\"Concat\"):\n",
    "            x = tf.concat([x, self.input_num], axis=1)\n",
    "\n",
    "        # Configure fc layers\n",
    "        for i, layer_units in enumerate(fc_layers):\n",
    "            with tf.name_scope(\"FullyConnected\"):\n",
    "                W = tf.Variable(initializer([x.get_shape()[-1].value, layer_units]),\n",
    "                                dtype='float32', name='W')\n",
    "                b = tf.Variable(initializer([layer_units]),\n",
    "                                dtype='float32', name='W')\n",
    "                x = tf.nn.xw_plus_b(x, W, b, name='fully-connected')\n",
    "                x = tf.nn.relu(x)\n",
    "\n",
    "            with tf.name_scope(\"Dropout\"):\n",
    "                x = tf.nn.dropout(x, self.dropout_keep_prob)\n",
    "\n",
    "        with tf.name_scope(\"Output\"):\n",
    "            W = tf.Variable(initializer([x.get_shape()[-1].value, 1]),\n",
    "                            dtype='float32', name='W')\n",
    "            b = tf.Variable(initializer([1]),\n",
    "                            dtype='float32', name='W')\n",
    "            self.yhat = tf.nn.xw_plus_b(x, W, b, name='output')\n",
    "\n",
    "        with tf.name_scope(\"Loss\"):\n",
    "#             yhat = tf.log(self.yhat + 1)\n",
    "#             y = tf.log(self.input_y + 1)\n",
    "#             diff = yhat - y\n",
    "#             self.loss = tf.sqrt(tf.reduce_mean(tf.square(diff)))\n",
    "\n",
    "            self.loss = tf.keras.metrics.mean_squared_logarithmic_error(self.yhat, self.input_y)\n",
    "#             self.loss = tf.sqrt(\n",
    "#                 tf.reduce_mean(\n",
    "#                     tf.square(\n",
    "#                         tf.subtract(tf.log(self.yhat + 1), tf.log(self.input_y + 1))\n",
    "#                     )\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#             self.loss = tf.reduce_mean(tf.square(self.yhat - self.input_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self, file_path, alstr, is_dev=False, batch_size=128, **args):\n",
    "        self.alstr = alstr\n",
    "        self.is_dev = is_dev\n",
    "        self.batch_size = batch_size\n",
    "        self.raw_data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "        self.alphabet = self.make_alphabet(self.alstr)\n",
    "        self.encoder, self.e_dict = self.one_hot_encoder(self.alphabet)\n",
    "        self.alphabet_size = len(self.alphabet)\n",
    "\n",
    "        self.x, self.y = self.format_data(self.raw_data)\n",
    "        self.input_x = self.x['desc_vecs'].values\n",
    "        self.input_num = self.x.drop(['desc_vecs'], axis=1).values\n",
    "            \n",
    "    def shuffling(self):\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(self.input_x)))\n",
    "        self.input_x = self.input_x[shuffle_indices]\n",
    "        self.input_num = self.input_num[shuffle_indices]\n",
    "        self.y = self.y[shuffle_indices]\n",
    "\n",
    "    def next_batch(self, batch_num):\n",
    "        data_size = len(self.input_x)\n",
    "        start = batch_num * self.batch_size\n",
    "        end = min((batch_num + 1) * self.batch_size, data_size)\n",
    "        batch_x = self.input_x[start:end]\n",
    "        batch_num = self.input_num[start:end]\n",
    "        batch_y = self.y[start:end]\n",
    "        return batch_x, batch_num, batch_y\n",
    "\n",
    "    def process_full_description(self, df):\n",
    "        df['desc_vecs'] = df['item_description'].apply(\n",
    "                lambda x: self.doc_process(x, self.e_dict)\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def categorinizer(self, df,\n",
    "                      col_lists=[\n",
    "                          'brand_name',\n",
    "                          'general_cat',\n",
    "                          'subcat_1',\n",
    "                          'subcat_2'\n",
    "                      ]):\n",
    "        for col in col_lists:\n",
    "            df[col] = df[col].apply(lambda x: str(x))\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(df[col])\n",
    "            df[col] = encoder.transform(df[col])\n",
    "            del encoder\n",
    "\n",
    "        return df\n",
    "\n",
    "    def split_cat(self, text):\n",
    "        try: return text.split(\"/\")\n",
    "        except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "    def format_data(self, df):\n",
    "        df['general_cat'], df['subcat_1'], df['subcat_2'] = \\\n",
    "            zip(*df['category_name'].apply(lambda x: self.split_cat(x)))\n",
    "\n",
    "        df = self.categorinizer(df)\n",
    "        # remove missing values in item description\n",
    "        df = df[pd.notnull(df['item_description'])]\n",
    "        df = self.process_full_description(df)\n",
    "        df['item_description'] = df['name'] + ' ' + df['item_description']\n",
    "        df = df.drop(columns=['name', 'category_name', 'item_description'])\n",
    "        if self.is_dev:\n",
    "            df = df.drop(columns=['test_id'])\n",
    "            price = None\n",
    "            features = df\n",
    "        else:\n",
    "            df = df.drop(columns=['train_id'])\n",
    "            price = df['price'].values\n",
    "            features = df.drop(columns=['price'])\n",
    "\n",
    "        return features, price\n",
    "\n",
    "    def one_hot_encoder(self, alphabet):\n",
    "        encoder_dict = {}\n",
    "        encoder = []\n",
    "\n",
    "        encoder_dict['UNK'] = 0\n",
    "        encoder.append(np.zeros(len(alphabet), dtype='float32'))\n",
    "\n",
    "        for i, alpha in enumerate(alphabet):\n",
    "            onehot = np.zeros(len(alphabet), dtype='float32')\n",
    "            encoder_dict[alpha] = i + 1\n",
    "            onehot[i] = 1\n",
    "            encoder.append(onehot)\n",
    "\n",
    "        encoder = np.array(encoder, dtype='float32')\n",
    "        return encoder, encoder_dict\n",
    "\n",
    "    def doc_process(self, desc, e_dict, l=200):\n",
    "        desc = desc.strip().lower()\n",
    "        min_len = min(l, len(desc))\n",
    "        doc_vec = np.zeros(l, dtype='int64')\n",
    "        for j in range(min_len):\n",
    "            if desc[j] in e_dict:\n",
    "                doc_vec[j] = e_dict[desc[j]]\n",
    "            else:\n",
    "                doc_vec[j] = e_dict['UNK']\n",
    "        return doc_vec\n",
    "\n",
    "    def make_alphabet(self, alstr):\n",
    "        return [char for char in alstr]\n",
    "\n",
    "# train = Data(train_file, config.alstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharNetConfig(object):\n",
    "    def __init__(self, params=None):\n",
    "        if params is None:\n",
    "            self.conv_layers = [\n",
    "                    [256, 7, 3],\n",
    "                    [256, 7, 3],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, 3]\n",
    "                ]\n",
    "            self.fc_layers = [1024, 1024]\n",
    "            self.l0 = 200\n",
    "            self.alstr = 'abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:’’’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}]'\n",
    "            self.alphabet_size = len(self.alstr)\n",
    "        else:\n",
    "            self.conv_layers = params['conv_layers']\n",
    "            self.fc_layers = params['fc_layers']\n",
    "            self.l0 = params['l0']\n",
    "            slef.alstr = params['alstr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os, time\n",
    "\n",
    "def main(train_file, dev_file, config):\n",
    "    with tf.Session() as sess:\n",
    "        charnet = CharNet(config.conv_layers, config.fc_layers, config.l0, config.alphabet_size, train_data.encoder)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads = optimizer.compute_gradients(charnet.loss)\n",
    "        train_op = optimizer.apply_gradients(grads)\n",
    "\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        # Summaries for grads\n",
    "    #     grad_summaries = []\n",
    "    #     for g, v in grads:\n",
    "    #         if g is not None:\n",
    "    #             grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "    #             sparsity_summary = tf.summary.histogram(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "    #             grad_summaries.append(grad_hist_summary)\n",
    "    #             grad_summaries.append(sparsity_summary)\n",
    "\n",
    "    #     grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", charnet.loss)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(num_batch, x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            y_batch = np.reshape(y_batch, (-1, 1))\n",
    "            mm = x_batch.tolist()\n",
    "            x_batch = np.array([k.tolist() for k in mm])\n",
    "            print(x_batch.shape)\n",
    "            feed_dict = {\n",
    "                charnet.input_num: num_batch,\n",
    "                charnet.input_x: x_batch,\n",
    "                charnet.input_y: y_batch,\n",
    "                charnet.dropout_keep_prob: .5\n",
    "            }\n",
    "\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op,\n",
    "                 global_step,\n",
    "                 train_summary_op,\n",
    "                 charnet.loss],\n",
    "                feed_dict\n",
    "            )\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def predict_on_test(num_batch, x_batch, results):\n",
    "            feed_dict = {\n",
    "                charnet.input_num: num_batch,\n",
    "                charnet.input_x: x_batch,\n",
    "                charnet.dropout_keep_prob: 1.0\n",
    "            }\n",
    "\n",
    "            results.append(sess.run([charnet.yhat], feed_dict))\n",
    "\n",
    "        for epoch in range(100):\n",
    "            train_data.shuffling()\n",
    "            for i in range(int(len(train_data.y)/train_data.batch_size) + 1):\n",
    "                input_x, input_num, y = train_data.next_batch(i)\n",
    "                train_step(input_num, input_x, y)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                if current_step % 100 == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "        results = []\n",
    "        predict_on_test(dev_data.input_num, dev_data.input_x)\n",
    "        submission = pd.DataFrame([[i for i in range(len(results))], results], columns=['id', 'price'])\n",
    "        submission.to_csv('./submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './train.tsv'\n",
    "dev_file = './test.tsv'\n",
    "config = CharNetConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arac/miniconda3/envs/gluon/lib/python3.6/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/arac/miniconda3/envs/gluon/lib/python3.6/site-packages/ipykernel_launcher.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "train_data = Data(train_file, config.alstr, is_dev=False)\n",
    "dev_data = Data(dev_file, config.alstr, is_dev=True)\n",
    "config = CharNetConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fd8177f38bfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     charnet = CharNet(config.conv_layers,\n\u001b[1;32m      4\u001b[0m                       \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                       \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "import os, time, datetime\n",
    "with tf.Session() as sess:\n",
    "    charnet = CharNet(config.conv_layers,\n",
    "                      config.fc_layers,\n",
    "                      config.l0,\n",
    "                      config.alphabet_size,\n",
    "                      train_data.encoder)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    grads = optimizer.compute_gradients(charnet.loss)\n",
    "    train_op = optimizer.apply_gradients(grads)\n",
    "    \n",
    "    train_op = optimizer.minimize(charnet.loss)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", charnet.loss)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train_step(num_batch, x_batch, y_batch, step):\n",
    "        \"\"\"\n",
    "        A single training step\n",
    "        \"\"\"\n",
    "        y_batch = np.reshape(y_batch, (-1, 1))\n",
    "        mm = x_batch.tolist()\n",
    "        x_batch = np.array([k.tolist() for k in mm])\n",
    "        \n",
    "        feed_dict = {\n",
    "            charnet.input_num: num_batch,\n",
    "            charnet.input_x: x_batch,\n",
    "            charnet.input_y: y_batch,\n",
    "            charnet.dropout_keep_prob: .5\n",
    "        }\n",
    "        \n",
    "        _, summaries, loss = sess.run(\n",
    "            [train_op,\n",
    "             train_summary_op,\n",
    "             charnet.loss],\n",
    "            feed_dict=feed_dict\n",
    "        )\n",
    "\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"step {}, loss {:g}\".format(step, loss))\n",
    "        train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "    def predict_on_test(num_batch, x_batch, results):\n",
    "        mm = x_batch.tolist()\n",
    "        x_batch = np.array([k.tolist() for k in mm])\n",
    "        feed_dict = {\n",
    "            charnet.input_num: num_batch,\n",
    "            charnet.input_x: x_batch,\n",
    "            charnet.dropout_keep_prob: 1.0\n",
    "        }\n",
    "        \n",
    "        results.append(sess.run([charnet.yhat], feed_dict=feed_dict))\n",
    "\n",
    "    for epoch in range(100):\n",
    "        train_data.shuffling()\n",
    "        for i in range(int(len(train_data.y)/train_data.batch_size) + 1):\n",
    "            input_x, input_num, y = train_data.next_batch(i)\n",
    "            train_step(input_num, input_x, y, i)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "    results = []\n",
    "    predict_on_test(dev_data.input_num, dev_data.input_x, results)\n",
    "    submission = pd.DataFrame([[i for i in range(len(results))], results], columns=['id', 'price'])\n",
    "    submission.to_csv('./submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f75057d1d95f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
