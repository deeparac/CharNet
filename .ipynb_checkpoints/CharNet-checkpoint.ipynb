{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_file = './train.tsv'\n",
    "\n",
    "train = pd.read_csv(train_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def categorinizer(df, \n",
    "                  col_lists=[\n",
    "                      'brand_name', \n",
    "                      'general_cat', \n",
    "                      'subcat_1', \n",
    "                      'subcat_2'\n",
    "                  ]):\n",
    "    for col in col_lists:\n",
    "        df[col] = \\\n",
    "            df[col].apply(lambda x: str(x))\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(df[col])\n",
    "        df[col] = encoder.transform(df[col])\n",
    "        del encoder\n",
    "    \n",
    "    return df\n",
    "\n",
    "def split_cat(text):\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "train['general_cat'], train['subcat_1'], train['subcat_2'] = \\\n",
    "    zip(*train['category_name'].apply(lambda x: split_cat(x)))\n",
    "\n",
    "# remove missing values in item description\n",
    "train = train[pd.notnull(train['item_description'])]\n",
    "\n",
    "train['item_description'] = train['name'] + ' ' + train['item_description']\n",
    "train_data = categorinizer(train)\n",
    "train_data = train_data.drop(columns=['name', 'category_name', 'train_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(alphabet):\n",
    "    encoder_dict = {}\n",
    "    encoder = []\n",
    "    \n",
    "    encoder_dict['UNK'] = 0\n",
    "    encoder.append(np.zeros(len(alphabet), dtype='float32'))\n",
    "    \n",
    "    for i, alpha in enumerate(alphabet):\n",
    "        onehot = np.zeros(len(alphabet), dtype='float32')\n",
    "        encoder_dict[alpha] = i + 1\n",
    "        onehot[i] = 1\n",
    "        encoder.append(onehot)\n",
    "    \n",
    "    encoder = np.array(encoder, dtype='float32')\n",
    "    return encoder, encoder_dict\n",
    "\n",
    "def doc_process(desc, e_dict, l=256):\n",
    "        desc = desc.strip().lower()\n",
    "        min_len = min(l, len(desc))\n",
    "        doc_vec = np.zeros(l, dtype='int64')\n",
    "        for j in range(min_len):\n",
    "            if desc[j] in e_dict:\n",
    "                doc_vec[j] = e_dict[desc[j]]\n",
    "            else:\n",
    "                doc_vec[j] = e_dict['UNK']\n",
    "        return doc_vec\n",
    "\n",
    "def make_alphabet(alstr='abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:’’’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}]'):\n",
    "    return [char for char in alstr]\n",
    "\n",
    "alphabet = make_alphabet()\n",
    "encoder, e_dict = one_hot_encoder(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18  1 26  5 18 58  2 12  1  3 11 23  9  4 15 23 58  3  8 18 15 13  1 58 11\n",
      "  5 25  2 15  1 18  4 58 20  8  9 19 58 11  5 25  2 15  1 18  4 58  9 19 58]\n",
      "(73, 72)\n"
     ]
    }
   ],
   "source": [
    "desc = train['item_description'][1]\n",
    "print(doc_process(desc, e_dict))\n",
    "print(encoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon.loss import Loss\n",
    "\n",
    "def embedding_lookup(params, ids):\n",
    "    # ids must be 2-D tensor [batch_size, encoded_words]\n",
    "    vecs = []\n",
    "    for i in ids:\n",
    "        vec = []\n",
    "        for j in i:\n",
    "            vec.append(params[j])\n",
    "        vecs.append(vec)\n",
    "    return list(reversed(vecs))\n",
    "\n",
    "class RMSLE(Loss):\n",
    "    def __init__(self, weight=1., batch_axis=0, **kwargs):\n",
    "        super(RMSLE, self).__init__(weight, batch_axis, **kwargs)\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, sample_weight=None):\n",
    "#         label = super()._reshape_like(F, label, pred)\n",
    "        loss = F.square(F.log(pred+1) - F.log(label+1))\n",
    "        loss = _apply_weighting(F, loss, self._weight/2, sample_weight)\n",
    "        return F.mean(loss, axis=self._batch_axis, exclude=True)\n",
    "\n",
    "\n",
    "def CharNet():\n",
    "    \"See Zhang and LeCun, 2015\"\n",
    "    \n",
    "    net = gluon.nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Conv1D(256, 7, activation='relu'))\n",
    "        net.add(gluon.nn.MaxPool1D(3, 1))\n",
    "        net.add(gluon.nn.Conv1D(256, 7, activation='relu'))\n",
    "        net.add(gluon.nn.MaxPool1D(3, 1))\n",
    "        net.add(gluon.nn.Conv1D(256, 3, activation='relu'))\n",
    "        net.add(gluon.nn.Conv1D(256, 3, activation='relu'))\n",
    "        net.add(gluon.nn.Conv1D(256, 3, activation='relu'))\n",
    "        net.add(gluon.nn.Conv1D(256, 3, activation='relu'))\n",
    "        net.add(gluon.nn.MaxPool1D(3, 1))\n",
    "        net.add(gluon.nn.Flatten())\n",
    "        net.add(gluon.nn.Dense(1024, activation=\"relu\"))\n",
    "        net.add(gluon.nn.Dropout(0.5))\n",
    "        net.add(gluon.nn.Dense(1024, activation=\"relu\"))\n",
    "        net.add(gluon.nn.Dropout(0.5))\n",
    "        net.add(gluon.nn.Dense(1))\n",
    "        \n",
    "    return net\n",
    "\n",
    "ctx = mx.cpu()\n",
    "model = CharNet()\n",
    "model.initialize(ctx=ctx)\n",
    "batch_size = 128\n",
    "rmsle = RMSLE()\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'wd': 1e-4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data['item_description'].apply(lambda x: doc_process(x, e_dict))\n",
    "X = np.array([x.tolist() for x in X]).tolist()\n",
    "X = embedding_lookup(encoder, X)\n",
    "\n",
    "y = train_data.price.values.tolist()\n",
    "\n",
    "training = gluon.data.DataLoader(gluon.data.ArrayDataset(X, y),\n",
    "                            batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "epochs = 10\n",
    "loss_sequence = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    # inner loop\n",
    "    for i, (data, label) in enumerate(training):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = model(data)\n",
    "            loss = rmsle(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        cumulative_loss += nd.mean(loss).asscalar()\n",
    "    print(\"Epoch %s, loss: %s\" % (e, cumulative_loss / num_examples))\n",
    "    loss_sequence.append(cumulative_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv1D(None -> 256, kernel_size=(7,), stride=(1,))\n",
      "  (1): MaxPool1D(size=(3,), stride=(1,), padding=(0,), ceil_mode=False)\n",
      "  (2): Conv1D(None -> 256, kernel_size=(7,), stride=(1,))\n",
      "  (3): MaxPool1D(size=(3,), stride=(1,), padding=(0,), ceil_mode=False)\n",
      "  (4): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (5): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (6): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (7): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (8): MaxPool1D(size=(3,), stride=(1,), padding=(0,), ceil_mode=False)\n",
      "  (9): Flatten\n",
      "  (10): Dense(None -> 1024, Activation(relu))\n",
      "  (11): Dropout(p = 0.5)\n",
      "  (12): Dense(None -> 1024, Activation(relu))\n",
      "  (13): Dropout(p = 0.5)\n",
      "  (14): Dense(None -> 1, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 72)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.nd.array(X[3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
