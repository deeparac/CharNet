{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class CharNet(object):\n",
    "    \"\"\"docstring for CharNet.\"\"\"\n",
    "    def __init__(self, conv_layers,\n",
    "                        fc_layers,\n",
    "                        l0,\n",
    "                        alphabet_size,\n",
    "                        encoder,\n",
    "                        **args\n",
    "    ):\n",
    "        super(CharNet, self).__init__()\n",
    "        tf.set_random_seed(time.time())\n",
    "        self.l0 = l0\n",
    "        self.conv_layers = conv_layers\n",
    "        self.fc_layers = fc_layers\n",
    "        self.alphabet_size = alphabet_size\n",
    "\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        with tf.name_scope('Input'):\n",
    "            self.input_num = tf.placeholder(tf.float32, shape=[None, 6],\n",
    "                                           name='input_num')\n",
    "            self.input_x = tf.placeholder(tf.int64, shape=[None, self.l0],\n",
    "                                          name='input_x')\n",
    "            self.input_y = tf.placeholder(tf.float32, shape=[None, 1],\n",
    "                                          name='input_y')\n",
    "            self.dropout_keep_prob = tf.placeholder(tf.float32,\n",
    "                                                    name='dropout_keep_prob')\n",
    "\n",
    "        with tf.name_scope('Embedding'), tf.device('/cpu:0'):\n",
    "            x = tf.nn.embedding_lookup(encoder, self.input_x)\n",
    "            x = tf.expand_dims(x, -1)\n",
    "\n",
    "        # Configure conv layers\n",
    "        for i, layer_params in enumerate(conv_layers):\n",
    "            with tf.name_scope(\"Convolution\"):\n",
    "                filter_param = [\n",
    "                    layer_params[1],\n",
    "                    x.get_shape()[2].value, # l0\n",
    "                    x.get_shape()[3].value, # channels\n",
    "                    layer_params[0]\n",
    "                ]\n",
    "                W = tf.Variable(initializer(filter_param), dtype='float32', name='filter')\n",
    "\n",
    "                conv_layer = tf.nn.conv2d(x, W, [1, 1, 1, 1], 'VALID', name='conv')\n",
    "                conv_layer = tf.nn.relu(conv_layer, name='act_relu')\n",
    "\n",
    "            if not layer_params[-1] is None:\n",
    "                with tf.name_scope(\"MaxPooling\"):\n",
    "                    pool_layer = tf.nn.max_pool(conv_layer,\n",
    "                                            ksize=[1, layer_params[-1], 1, 1],\n",
    "                                            strides=[1, layer_params[-1], 1, 1],\n",
    "                                            padding='VALID')\n",
    "                    x = tf.transpose(pool_layer, [0, 1, 3, 2])\n",
    "            else:\n",
    "                x = tf.transpose(conv_layer, [0, 1, 3, 2])\n",
    "\n",
    "        # flatten conv output for fc\n",
    "        with tf.name_scope(\"Flatten\"):\n",
    "            x = tf.contrib.layers.flatten(x)\n",
    "\n",
    "        with tf.name_scope(\"Concat\"):\n",
    "            x = tf.concat([x, self.input_num], axis=1)\n",
    "\n",
    "        # Configure fc layers\n",
    "        for i, layer_units in enumerate(fc_layers):\n",
    "            with tf.name_scope(\"FullyConnected\"):\n",
    "                W = tf.Variable(initializer([x.get_shape()[-1].value, layer_units]),\n",
    "                                dtype='float32', name='W')\n",
    "                b = tf.Variable(initializer([layer_units]),\n",
    "                                dtype='float32', name='W')\n",
    "                x = tf.nn.xw_plus_b(x, W, b, name='fully-connected')\n",
    "                x = tf.nn.relu(x)\n",
    "\n",
    "            with tf.name_scope(\"Dropout\"):\n",
    "                x = tf.nn.dropout(x, self.dropout_keep_prob)\n",
    "\n",
    "        with tf.name_scope(\"Output\"):\n",
    "            W = tf.Variable(initializer([x.get_shape()[-1].value, 1]),\n",
    "                            dtype='float32', name='W')\n",
    "            b = tf.Variable(initializer([1]),\n",
    "                            dtype='float32', name='W')\n",
    "            self.yhat = tf.nn.xw_plus_b(x, W, b, name='output')\n",
    "\n",
    "        with tf.name_scope(\"Loss\"):\n",
    "#             yhat = tf.log(self.yhat + 1)\n",
    "#             y = tf.log(self.input_y + 1)\n",
    "#             diff = yhat - y\n",
    "#             self.loss = tf.sqrt(tf.reduce_mean(tf.square(diff)))\n",
    "            yhat = tf.reshape(self.yhat, [-1])\n",
    "            y = tf.reshape(self.input_y, [-1])\n",
    "            self.loss = tf.keras.metrics.mean_squared_logarithmic_error(yhat, y)\n",
    "#       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self, file_path, alstr, is_dev=False, batch_size=128, **args):\n",
    "        self.alstr = alstr\n",
    "        self.is_dev = is_dev\n",
    "        self.batch_size = batch_size\n",
    "        self.raw_data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "        self.alphabet = self.make_alphabet(self.alstr)\n",
    "        self.encoder, self.e_dict = self.one_hot_encoder(self.alphabet)\n",
    "        self.alphabet_size = len(self.alphabet)\n",
    "\n",
    "        self.x, self.y = self.format_data(self.raw_data)\n",
    "        self.input_x = self.x['desc_vecs'].values\n",
    "        self.input_num = self.x.drop(['desc_vecs'], axis=1).values\n",
    "            \n",
    "    def shuffling(self):\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(self.input_x)))\n",
    "        self.input_x = self.input_x[shuffle_indices]\n",
    "        self.input_num = self.input_num[shuffle_indices]\n",
    "        self.y = self.y[shuffle_indices]\n",
    "\n",
    "    def next_batch(self, batch_num):\n",
    "        data_size = len(self.input_x)\n",
    "        start = batch_num * self.batch_size\n",
    "        end = min((batch_num + 1) * self.batch_size, data_size)\n",
    "        batch_x = self.input_x[start:end]\n",
    "        batch_num = self.input_num[start:end]\n",
    "        batch_y = self.y[start:end]\n",
    "        return batch_x, batch_num, batch_y\n",
    "\n",
    "    def process_full_description(self, df):\n",
    "        df['desc_vecs'] = df['item_description'].apply(\n",
    "                lambda x: self.doc_process(x, self.e_dict)\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def categorinizer(self, df,\n",
    "                      col_lists=[\n",
    "                          'brand_name',\n",
    "                          'general_cat',\n",
    "                          'subcat_1',\n",
    "                          'subcat_2'\n",
    "                      ]):\n",
    "        for col in col_lists:\n",
    "            df[col] = df[col].apply(lambda x: str(x))\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(df[col])\n",
    "            df[col] = encoder.transform(df[col])\n",
    "            del encoder\n",
    "\n",
    "        return df\n",
    "\n",
    "    def split_cat(self, text):\n",
    "        try: return text.split(\"/\")\n",
    "        except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "    def format_data(self, df):\n",
    "        df['general_cat'], df['subcat_1'], df['subcat_2'] = \\\n",
    "            zip(*df['category_name'].apply(lambda x: self.split_cat(x)))\n",
    "\n",
    "        df = self.categorinizer(df)\n",
    "        # remove missing values in item description\n",
    "        df = df[pd.notnull(df['item_description'])]\n",
    "        df = self.process_full_description(df)\n",
    "        df['item_description'] = df['name'] + ' ' + df['item_description']\n",
    "        df = df.drop(columns=['name', 'category_name', 'item_description'])\n",
    "        if self.is_dev:\n",
    "            df = df.drop(columns=['test_id'])\n",
    "            price = None\n",
    "            features = df\n",
    "        else:\n",
    "            df = df.drop(columns=['train_id'])\n",
    "            price = df['price'].values\n",
    "            features = df.drop(columns=['price'])\n",
    "\n",
    "        return features, price\n",
    "\n",
    "    def one_hot_encoder(self, alphabet):\n",
    "        encoder_dict = {}\n",
    "        encoder = []\n",
    "\n",
    "        encoder_dict['UNK'] = 0\n",
    "        encoder.append(np.zeros(len(alphabet), dtype='float32'))\n",
    "\n",
    "        for i, alpha in enumerate(alphabet):\n",
    "            onehot = np.zeros(len(alphabet), dtype='float32')\n",
    "            encoder_dict[alpha] = i + 1\n",
    "            onehot[i] = 1\n",
    "            encoder.append(onehot)\n",
    "\n",
    "        encoder = np.array(encoder, dtype='float32')\n",
    "        return encoder, encoder_dict\n",
    "\n",
    "    def doc_process(self, desc, e_dict, l=200):\n",
    "        desc = desc.strip().lower()\n",
    "        min_len = min(l, len(desc))\n",
    "        doc_vec = np.zeros(l, dtype='int64')\n",
    "        for j in range(min_len):\n",
    "            if desc[j] in e_dict:\n",
    "                doc_vec[j] = e_dict[desc[j]]\n",
    "            else:\n",
    "                doc_vec[j] = e_dict['UNK']\n",
    "        return doc_vec\n",
    "\n",
    "    def make_alphabet(self, alstr):\n",
    "        return [char for char in alstr]\n",
    "\n",
    "# train = Data(train_file, config.alstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharNetConfig(object):\n",
    "    def __init__(self, params=None):\n",
    "        if params is None:\n",
    "            self.conv_layers = [\n",
    "                    [256, 7, 3],\n",
    "                    [256, 7, 3],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, 3]\n",
    "                ]\n",
    "            self.fc_layers = [1024, 1024]\n",
    "            self.l0 = 200\n",
    "            self.alstr = 'abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:’’’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}]'\n",
    "            self.alphabet_size = len(self.alstr)\n",
    "        else:\n",
    "            self.conv_layers = params['conv_layers']\n",
    "            self.fc_layers = params['fc_layers']\n",
    "            self.l0 = params['l0']\n",
    "            slef.alstr = params['alstr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os, time\n",
    "\n",
    "def main(train_file, dev_file, config):\n",
    "    with tf.Session() as sess:\n",
    "        charnet = CharNet(config.conv_layers, config.fc_layers, config.l0, config.alphabet_size, train_data.encoder)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads = optimizer.compute_gradients(charnet.loss)\n",
    "        train_op = optimizer.apply_gradients(grads)\n",
    "\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        # Summaries for grads\n",
    "    #     grad_summaries = []\n",
    "    #     for g, v in grads:\n",
    "    #         if g is not None:\n",
    "    #             grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "    #             sparsity_summary = tf.summary.histogram(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "    #             grad_summaries.append(grad_hist_summary)\n",
    "    #             grad_summaries.append(sparsity_summary)\n",
    "\n",
    "    #     grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", charnet.loss)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(num_batch, x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            y_batch = np.reshape(y_batch, (-1, 1))\n",
    "            mm = x_batch.tolist()\n",
    "            x_batch = np.array([k.tolist() for k in mm])\n",
    "            print(x_batch.shape)\n",
    "            feed_dict = {\n",
    "                charnet.input_num: num_batch,\n",
    "                charnet.input_x: x_batch,\n",
    "                charnet.input_y: y_batch,\n",
    "                charnet.dropout_keep_prob: .5\n",
    "            }\n",
    "\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op,\n",
    "                 global_step,\n",
    "                 train_summary_op,\n",
    "                 charnet.loss],\n",
    "                feed_dict\n",
    "            )\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def predict_on_test(num_batch, x_batch, results):\n",
    "            feed_dict = {\n",
    "                charnet.input_num: num_batch,\n",
    "                charnet.input_x: x_batch,\n",
    "                charnet.dropout_keep_prob: 1.0\n",
    "            }\n",
    "\n",
    "            results.append(sess.run([charnet.yhat], feed_dict))\n",
    "\n",
    "        for epoch in range(100):\n",
    "            train_data.shuffling()\n",
    "            for i in range(int(len(train_data.y)/train_data.batch_size) + 1):\n",
    "                input_x, input_num, y = train_data.next_batch(i)\n",
    "                train_step(input_num, input_x, y)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                if current_step % 100 == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "        results = []\n",
    "        predict_on_test(dev_data.input_num, dev_data.input_x)\n",
    "        submission = pd.DataFrame([[i for i in range(len(results))], results], columns=['id', 'price'])\n",
    "        submission.to_csv('./submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './train.tsv'\n",
    "dev_file = './test.tsv'\n",
    "config = CharNetConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arac/miniconda3/envs/gluon/lib/python3.6/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/arac/miniconda3/envs/gluon/lib/python3.6/site-packages/ipykernel_launcher.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "train_data = Data(train_file, config.alstr, is_dev=False)\n",
    "dev_data = Data(dev_file, config.alstr, is_dev=True)\n",
    "config = CharNetConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/arac/aramac/CharNet/runs/1514534076\n",
      "\n",
      "step 0, loss nan\n",
      "Saved model checkpoint to /home/arac/aramac/CharNet/runs/1514534076/checkpoints/model-0\n",
      "\n",
      "step 1, loss nan\n",
      "step 2, loss nan\n",
      "step 3, loss nan\n",
      "step 4, loss nan\n",
      "step 5, loss nan\n",
      "step 6, loss nan\n",
      "step 7, loss nan\n",
      "step 8, loss nan\n",
      "step 9, loss nan\n",
      "step 10, loss nan\n",
      "step 11, loss nan\n",
      "step 12, loss nan\n",
      "step 13, loss nan\n",
      "step 14, loss nan\n",
      "step 15, loss nan\n",
      "step 16, loss nan\n",
      "step 17, loss nan\n",
      "step 18, loss nan\n",
      "step 19, loss nan\n",
      "step 20, loss nan\n",
      "step 21, loss nan\n",
      "step 22, loss nan\n",
      "step 23, loss nan\n",
      "step 24, loss nan\n",
      "step 25, loss nan\n",
      "step 26, loss nan\n",
      "step 27, loss nan\n",
      "step 28, loss nan\n",
      "step 29, loss nan\n",
      "step 30, loss nan\n",
      "step 31, loss nan\n",
      "step 32, loss nan\n",
      "step 33, loss nan\n",
      "step 34, loss nan\n",
      "step 35, loss nan\n",
      "step 36, loss nan\n",
      "step 37, loss nan\n",
      "step 38, loss nan\n",
      "step 39, loss nan\n",
      "step 40, loss nan\n",
      "step 41, loss nan\n",
      "step 42, loss nan\n",
      "step 43, loss nan\n",
      "step 44, loss nan\n",
      "step 45, loss nan\n",
      "step 46, loss nan\n",
      "step 47, loss nan\n",
      "step 48, loss nan\n",
      "step 49, loss nan\n",
      "step 50, loss nan\n",
      "step 51, loss nan\n",
      "step 52, loss nan\n",
      "step 53, loss nan\n",
      "step 54, loss nan\n",
      "step 55, loss nan\n",
      "step 56, loss nan\n",
      "step 57, loss nan\n",
      "step 58, loss nan\n",
      "step 59, loss nan\n",
      "step 60, loss nan\n",
      "step 61, loss nan\n",
      "step 62, loss nan\n",
      "step 63, loss nan\n",
      "step 64, loss nan\n",
      "step 65, loss nan\n",
      "step 66, loss nan\n",
      "step 67, loss nan\n",
      "step 68, loss nan\n",
      "step 69, loss nan\n",
      "step 70, loss nan\n",
      "step 71, loss nan\n",
      "step 72, loss nan\n",
      "step 73, loss nan\n",
      "step 74, loss nan\n",
      "step 75, loss nan\n",
      "step 76, loss nan\n",
      "step 77, loss nan\n",
      "step 78, loss nan\n",
      "step 79, loss nan\n",
      "step 80, loss nan\n",
      "step 81, loss nan\n",
      "step 82, loss nan\n",
      "step 83, loss nan\n",
      "step 84, loss nan\n",
      "step 85, loss nan\n",
      "step 86, loss nan\n",
      "step 87, loss nan\n",
      "step 88, loss nan\n",
      "step 89, loss nan\n",
      "step 90, loss nan\n",
      "step 91, loss nan\n",
      "step 92, loss nan\n",
      "step 93, loss nan\n",
      "step 94, loss nan\n",
      "step 95, loss nan\n",
      "step 96, loss nan\n",
      "step 97, loss nan\n",
      "step 98, loss nan\n",
      "step 99, loss nan\n",
      "step 100, loss nan\n",
      "step 101, loss nan\n",
      "step 102, loss nan\n",
      "step 103, loss nan\n",
      "step 104, loss nan\n",
      "step 105, loss nan\n",
      "step 106, loss nan\n",
      "step 107, loss nan\n",
      "step 108, loss nan\n",
      "step 109, loss nan\n",
      "step 110, loss nan\n",
      "step 111, loss nan\n",
      "step 112, loss nan\n",
      "step 113, loss nan\n",
      "step 114, loss nan\n",
      "step 115, loss nan\n",
      "step 116, loss nan\n",
      "step 117, loss nan\n",
      "step 118, loss nan\n",
      "step 119, loss nan\n",
      "step 120, loss nan\n",
      "step 121, loss nan\n",
      "step 122, loss nan\n",
      "step 123, loss nan\n",
      "step 124, loss nan\n",
      "step 125, loss nan\n",
      "step 126, loss nan\n",
      "step 127, loss nan\n",
      "step 128, loss nan\n",
      "step 129, loss nan\n",
      "step 130, loss nan\n",
      "step 131, loss nan\n",
      "step 132, loss nan\n",
      "step 133, loss nan\n",
      "step 134, loss nan\n",
      "step 135, loss nan\n",
      "step 136, loss nan\n",
      "step 137, loss nan\n",
      "step 138, loss nan\n",
      "step 139, loss nan\n",
      "step 140, loss nan\n",
      "step 141, loss nan\n",
      "step 142, loss nan\n",
      "step 143, loss nan\n",
      "step 144, loss nan\n",
      "step 145, loss nan\n",
      "step 146, loss nan\n",
      "step 147, loss nan\n",
      "step 148, loss nan\n",
      "step 149, loss nan\n",
      "step 150, loss nan\n",
      "step 151, loss nan\n",
      "step 152, loss nan\n",
      "step 153, loss nan\n",
      "step 154, loss nan\n",
      "step 155, loss nan\n",
      "step 156, loss nan\n",
      "step 157, loss nan\n",
      "step 158, loss nan\n",
      "step 159, loss nan\n",
      "step 160, loss nan\n",
      "step 161, loss nan\n",
      "step 162, loss nan\n",
      "step 163, loss nan\n",
      "step 164, loss nan\n",
      "step 165, loss nan\n",
      "step 166, loss nan\n",
      "step 167, loss nan\n",
      "step 168, loss nan\n",
      "step 169, loss nan\n",
      "step 170, loss nan\n",
      "step 171, loss nan\n",
      "step 172, loss nan\n",
      "step 173, loss nan\n",
      "step 174, loss nan\n",
      "step 175, loss nan\n",
      "step 176, loss nan\n",
      "step 177, loss nan\n",
      "step 178, loss nan\n",
      "step 179, loss nan\n",
      "step 180, loss nan\n",
      "step 181, loss nan\n",
      "step 182, loss nan\n",
      "step 183, loss nan\n",
      "step 184, loss nan\n",
      "step 185, loss nan\n",
      "step 186, loss nan\n",
      "step 187, loss nan\n",
      "step 188, loss nan\n",
      "step 189, loss nan\n",
      "step 190, loss nan\n",
      "step 191, loss nan\n",
      "step 192, loss nan\n",
      "step 193, loss nan\n",
      "step 194, loss nan\n",
      "step 195, loss nan\n",
      "step 196, loss nan\n",
      "step 197, loss nan\n",
      "step 198, loss nan\n",
      "step 199, loss nan\n",
      "step 200, loss nan\n",
      "step 201, loss nan\n",
      "step 202, loss nan\n",
      "step 203, loss nan\n",
      "step 204, loss nan\n",
      "step 205, loss nan\n",
      "step 206, loss nan\n",
      "step 207, loss nan\n",
      "step 208, loss nan\n",
      "step 209, loss nan\n",
      "step 210, loss nan\n",
      "step 211, loss nan\n",
      "step 212, loss nan\n",
      "step 213, loss nan\n",
      "step 214, loss nan\n",
      "step 215, loss nan\n",
      "step 216, loss nan\n",
      "step 217, loss nan\n",
      "step 218, loss nan\n",
      "step 219, loss nan\n",
      "step 220, loss nan\n",
      "step 221, loss nan\n",
      "step 222, loss nan\n",
      "step 223, loss nan\n",
      "step 224, loss nan\n",
      "step 225, loss nan\n",
      "step 226, loss nan\n",
      "step 227, loss nan\n",
      "step 228, loss nan\n",
      "step 229, loss nan\n",
      "step 230, loss nan\n",
      "step 231, loss nan\n",
      "step 232, loss nan\n",
      "step 233, loss nan\n",
      "step 234, loss nan\n",
      "step 235, loss nan\n",
      "step 236, loss nan\n",
      "step 237, loss nan\n",
      "step 238, loss nan\n",
      "step 239, loss nan\n",
      "step 240, loss nan\n",
      "step 241, loss nan\n",
      "step 242, loss nan\n",
      "step 243, loss nan\n",
      "step 244, loss nan\n",
      "step 245, loss nan\n",
      "step 246, loss nan\n",
      "step 247, loss nan\n",
      "step 248, loss nan\n",
      "step 249, loss nan\n",
      "step 250, loss nan\n",
      "step 251, loss nan\n",
      "step 252, loss nan\n",
      "step 253, loss nan\n",
      "step 254, loss nan\n",
      "step 255, loss nan\n",
      "step 256, loss nan\n",
      "step 257, loss nan\n",
      "step 258, loss nan\n",
      "step 259, loss nan\n",
      "step 260, loss nan\n",
      "step 261, loss nan\n",
      "step 262, loss nan\n",
      "step 263, loss nan\n",
      "step 264, loss nan\n",
      "step 265, loss nan\n",
      "step 266, loss nan\n",
      "step 267, loss nan\n",
      "step 268, loss nan\n",
      "step 269, loss nan\n",
      "step 270, loss nan\n",
      "step 271, loss nan\n",
      "step 272, loss nan\n",
      "step 273, loss nan\n",
      "step 274, loss nan\n",
      "step 275, loss nan\n",
      "step 276, loss nan\n",
      "step 277, loss nan\n",
      "step 278, loss nan\n",
      "step 279, loss nan\n",
      "step 280, loss nan\n",
      "step 281, loss nan\n",
      "step 282, loss nan\n",
      "step 283, loss nan\n",
      "step 284, loss nan\n",
      "step 285, loss nan\n",
      "step 286, loss nan\n",
      "step 287, loss nan\n",
      "step 288, loss nan\n",
      "step 289, loss nan\n",
      "step 290, loss nan\n",
      "step 291, loss nan\n",
      "step 292, loss nan\n",
      "step 293, loss nan\n",
      "step 294, loss nan\n",
      "step 295, loss nan\n",
      "step 296, loss nan\n",
      "step 297, loss nan\n",
      "step 298, loss nan\n",
      "step 299, loss nan\n",
      "step 300, loss nan\n",
      "step 301, loss nan\n",
      "step 302, loss nan\n",
      "step 303, loss nan\n",
      "step 304, loss nan\n",
      "step 305, loss nan\n",
      "step 306, loss nan\n",
      "step 307, loss nan\n",
      "step 308, loss nan\n",
      "step 309, loss nan\n",
      "step 310, loss nan\n",
      "step 311, loss nan\n",
      "step 312, loss nan\n",
      "step 313, loss nan\n",
      "step 314, loss nan\n",
      "step 315, loss nan\n",
      "step 316, loss nan\n",
      "step 317, loss nan\n",
      "step 318, loss nan\n",
      "step 319, loss nan\n",
      "step 320, loss nan\n",
      "step 321, loss nan\n",
      "step 322, loss nan\n",
      "step 323, loss nan\n",
      "step 324, loss nan\n",
      "step 325, loss nan\n",
      "step 326, loss nan\n",
      "step 327, loss nan\n",
      "step 328, loss nan\n",
      "step 329, loss nan\n",
      "step 330, loss nan\n",
      "step 331, loss nan\n",
      "step 332, loss nan\n",
      "step 333, loss nan\n",
      "step 334, loss nan\n",
      "step 335, loss nan\n",
      "step 336, loss nan\n",
      "step 337, loss nan\n",
      "step 338, loss nan\n",
      "step 339, loss nan\n",
      "step 340, loss nan\n",
      "step 341, loss nan\n",
      "step 342, loss nan\n",
      "step 343, loss nan\n",
      "step 344, loss nan\n",
      "step 345, loss nan\n",
      "step 346, loss nan\n",
      "step 347, loss nan\n",
      "step 348, loss nan\n",
      "step 349, loss nan\n",
      "step 350, loss nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-21beeffd8772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0minput_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-21beeffd8772>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(num_batch, x_batch, y_batch, step)\u001b[0m\n\u001b[1;32m     54\u001b[0m              \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m              charnet.loss],\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         )\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#         yhat, y = sess.run([charnet.yhat, charnet.input_y], feed_dict=feed_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gluon/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gluon/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gluon/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gluon/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gluon/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, time, datetime\n",
    "with tf.Session() as sess:\n",
    "    charnet = CharNet(config.conv_layers,\n",
    "                      config.fc_layers,\n",
    "                      config.l0,\n",
    "                      config.alphabet_size,\n",
    "                      train_data.encoder)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    grads = optimizer.compute_gradients(charnet.loss)\n",
    "    train_op = optimizer.apply_gradients(grads)\n",
    "    \n",
    "    train_op = optimizer.minimize(charnet.loss)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", charnet.loss)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train_step(num_batch, x_batch, y_batch, step):\n",
    "        \"\"\"\n",
    "        A single training step\n",
    "        \"\"\"\n",
    "        y_batch = np.reshape(y_batch, (-1, 1))\n",
    "        mm = x_batch.tolist()\n",
    "        x_batch = np.array([k.tolist() for k in mm])\n",
    "        \n",
    "        feed_dict = {\n",
    "            charnet.input_num: num_batch,\n",
    "            charnet.input_x: x_batch,\n",
    "            charnet.input_y: y_batch,\n",
    "            charnet.dropout_keep_prob: .5\n",
    "        }\n",
    "        \n",
    "        _, summaries, loss = sess.run(\n",
    "            [train_op,\n",
    "             train_summary_op,\n",
    "             charnet.loss],\n",
    "            feed_dict=feed_dict\n",
    "        )\n",
    "#         yhat, y = sess.run([charnet.yhat, charnet.input_y], feed_dict=feed_dict)\n",
    "#         print(yhat.shape)\n",
    "#         print(y.shape)\n",
    "#         time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"step {}, loss {:g}\".format(step, loss))\n",
    "        train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "    def predict_on_test(num_batch, x_batch, results):\n",
    "        mm = x_batch.tolist()\n",
    "        x_batch = np.array([k.tolist() for k in mm])\n",
    "        feed_dict = {\n",
    "            charnet.input_num: num_batch,\n",
    "            charnet.input_x: x_batch,\n",
    "            charnet.dropout_keep_prob: 1.0\n",
    "        }\n",
    "        \n",
    "        results.append(sess.run([charnet.yhat], feed_dict=feed_dict))\n",
    "    \n",
    "#     input_x, input_num, y = train_data.next_batch(i)\n",
    "#     train_step(input_num, input_x, y, 0)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        train_data.shuffling()\n",
    "        for i in range(int(len(train_data.y)/train_data.batch_size) + 1):\n",
    "            input_x, input_num, y = train_data.next_batch(i)\n",
    "            train_step(input_num, input_x, y, i)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "#     results = []\n",
    "#     predict_on_test(dev_data.input_num, dev_data.input_x, results)\n",
    "#     submission = pd.DataFrame([[i for i in range(len(results))], results], columns=['id', 'price'])\n",
    "#     submission.to_csv('./submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
